\newpage

\begin{CJK*}{GBK}{song}

\setcounter{chapter}{1}
\setcounter{section}{0}

\chapter{计算广告基础}

\thispagestyle{empty}
\markboth{计算广告基础}{计算广告基础}

通过上一章的介绍，大家应该对在线广告业务上的本质和市场发展脉络有了初步的认识。本章的重点，是介绍在线广告中计算到底是为了解决哪些问题，以及解决这些问题所需要的系统框架与算法基础知识。

在线广告中计算之所以可以发挥巨大的作用，与它的一些根本技术特点有很大关系，这是本章的出发点。在这些技术特点的基础上，我们对Andrei Broder提出的计算广告核心挑战稍作推广，得到贯穿本书的计算广告核心问题，即ROI优化问题的概念性框架表达。这一框架除了可以生发后面许多的计算问题，还对在线广告市场结构和计费方式的一致理解很有帮助，我们也对这一点做了讨论。

为了方便后文中各种广告产品中的系统架构和算法讨论，我们在本章中将进行两方面的准备工作：一是给出一个示意性的广告系统统一架构，虽然这一架构的各个组成模块在不同的广告产品中各有取舍和变形，却十分有利于我们从宏观上把握广告系统的全貌和各种产品在技术方面的内在联系。后文各种广告产品的架构讨论，都会在这个统一框架的基础上进行。二是对计算广告中广泛使用的一些基础知识，如信息检索、最优化方法、统计及其学习等，作提纲挈领式的简要介绍。这样的是为了在后文讨论到相关算法时，可以有相对统一的展开基础，并避免在广告专门算法的讨论上穿插过大篇幅的基础内容。如果读者对某部分得知市已有相当的基础，则可以不做详细阅读，只简单了解本书的一些惯例数学表达；如果觉得某部分需要深入了解，那么还需要根据本章的简要提纲，参照其他专门文献获得更全面和深入的内容。

\section{在线广告的技术特点}

从上一章的在线广告简史中，大家肯定已经发现了不少在线广告不同于传统广告的特点。在这些不同点当中，有一些对我们正确理解在线广告市场，对该市场的效果优化探究合适的解决方案，有着非常重要的指导意义。我们把这些重要特点总结出来，以便引出计算广告的核心问题与框架。

一、技术和计算导向。数字媒体的特点使在线广告可以进行精细的受众定向，技术又使得广告决策和交易朝着计算驱动的方向发展。之所以有这一特点，我们才会在这里讨论煞有介事地计算广告。实际上，受众定向这一思想在线下广告中也曾经被尝试过，比如试图把信用卡纸质帐单背面的广告按照信用卡用户的年龄和性别做一些定制化，不过由于非数字的媒体上这么做的成本太高，因而无法规模化。在数字媒体上进行受众定向，其成本可以控制得非常低，这也直接催生了在线广告的计算革命。

二、效果的可衡量性。在线广告刚刚产生的时候，大家对这种广告最多的称道之处，是它可以以展示和点击日志的形式直接记录广告效果。当然，我们也可以利用这些日志优化广告效果，这同样是计算广告非常重要的方法论。不过，点击率这一指标从是否在绝对意义上能够反映广告效果，个人认为值得探讨。从98年到今天，条幅显示广告的点击率从10\%逐渐降低到降至0.1\%，难道这说明广告的效果下降了两个数量级么？快速增长的显示广告市场规模显然给出了否定的回答。我们认为，在同一个时期，点击率的绝对值并没有那么重要，而在一个特定时期不同广告和算法表现出来的差异，才是更有意义的。从这一点来看，可衡量性仍然可以认为是在线广告的一个根本特点。

三、创意和投放方式的标准化。标准化的驱动力来自于受众定向与程序购买。既然需求方关心的是人群而非广告位，创意尺寸的统一化与一些关键接口的标准化非常关键。这些接口标准中，比较典型的有上一章中介绍的视频广告的VAST标准和实时竞价的Open RTB标准等。

四、媒体概念的多样化。随着Web 2.0的普及，赋予了更多交互功能的互联网媒体与线下媒体有大有不同。随着交互功能的不同，这些媒体与转化行为的距离也就不同。以电商行业为利，门户网站、垂直网站、搜索引擎、电商网站、返利网，在转化链条上一个比一个更靠近购买行为。越接近需求方的媒体上的广告，一定可以达到越高的ROI，不过离“引导潜在用户”这样的广告目的也就越远。因此我们在从需求方看在线广告时，应该注重各种性质媒体的配合关系，并从整合营销的角度去审视和优化整体的效果。试想，如果一家电商只用返利网作为线上广告渠道，ROI一定可以做到很高，可是返利网能给他带来潜在用户么？

\section{计算广告核心问题}

Andrei Broder在提出计算广告这一概念的时候，也给出了该课题的核心研究问题。这一核心问题，他的表述是“Find the best match between a given user, a given context, and a suitable ad”。我们结合近年来市场的发展以及实际业务中的体会，对此表述稍做加工如下：

\begin{CJK*}{GBK}{kai}计算广告的核心问题，是为一系列用户与环境的组合，找到最合适的广告投放策略以优化整体的投入产出比(ROI)。\end{CJK*}

与Andrei Broder的表述，我们主要进行的两方面的微调：一、我们强调这一问题优化的是一组展示上的效果，而非孤立的某一次展示上的效果。这是由于广告活动中普遍存在着量的约束，在这一约束下进行ROI优化，其最优解往往与每次展示独立决策时有很大的不同；二、描述中去掉了"given"的字眼，这是由于在某些广告产品中，并不一定能拿到确定的用户或上下文标识，但这并不意味着完全无法进行计算优化。同样地，我们也强调优化的结果是“广告投放策略”而不一定是具体的广告，这也是因为有些产品的策略并不是直接决定最后的展示。这些看来古怪的咬文嚼字，其实只是做一个铺垫，相信读完本书后面的部分，大家就能更深入地体会做这些调整的原因。

这个核心的挑战，可以概念上表达为下面的最优化问题：

\begin{equation}\label{opt}
\max_{a_{1,\cdots,T}}\sum_{i=1}^T r(a_i, u_i, c_i) \bigg / \sum_{i=1}^T p_i
\end{equation}

表达式中的$a, u, c$三个变量，分别代表广告、用户与上下文，即广告活动的三个参与主体。$i$代表从第1次到第$T$次之间的某一次广告展示。我们优化的目标，就是在这$T$次展示上的总产出($r$)与总投入($p$)的比，即ROI。注意这里有一个隐含的假设，即整体的ROI可以被分解到每次展示上。这一假设显然是不太合理的，但是考虑到实际线上决策时，必须对每个impression马上完成计算，所以从实用出发我们仍然采用这一假设，而用频次特征之类的方法解决impression之间相关性的问题。

下面我们再来看看ROI进一步的具体分解方式，以便引出关于在线广告市场上几种收费方式的重要的讨论。对一个广告市场中具体的产品形态而言，我们往往能够主动优化的是产出(return)而非投入(investment)的部分。一次广告展示产生后，会发生哪些后续行为呢？首先是点击，这是在媒体网站上发生的；然后是转化，这是在广告主网站上或线下发生的。按照点击和转化两个阶段对return进行分解，使实践中合理且容易操作的方式：
\begin{equation}
r(a, u, c) = \mu(a, u, c) \cdot \nu(a, u)
\end{equation}
在后文中，我们都沿用这样的表示：$\mu$表示\begin{CJK*}{GBK}{kai}{点击率(Click through Rate, CTR)}\end{CJK*}，$\nu$表示\begin{CJK*}{GBK}{kai}{点击价值(Click Value)}\end{CJK*}，而这两部分的乘积，即定量地表示了某次或若干次展示的期望CPM值，我们称之为expected CPM(eCPM)。eCPM是计算广告中最常被提及，也最有代表性的定量评估收益的指标，本书中有大量的计算问题都是围绕它展开的。实际上，点击价值是还可以进一步分解的，比如对于购物的网站，可以分解为到达率、\begin{CJK*}{GBK}{kai}{转化率(Conversion Rate)}\end{CJK*}与客单价的乘积。不过这部分的深入解剖与行业密切相关，而且更多地属于站内运营而非广告的范畴，因此在本书中将不做详细讨论。与这里的分析相对应，我们认为点击率是$(a,u,c)$三者的函数，而转化率可以近似为$(a,c)$的函数。

对于大多数广告产品而言，对需要得到给定$(a, u, c)$三元组的eCPM以便进行决策。可是由于广告市场的协作关系复杂，并非每个广告产品都可以对eCPM中的两部分做出较准确的估计。根据eCPM的分解决定哪部分由谁来估计，是广告市场各种计费模式产生的根本原因。我们来看一下三种主要的计费模式：

一、CPM计费：这种方式是供给方与需求方约定好千次展示的计费标准，至于这些展示是否能够带来相应的收益，由需求方来估计和控制其中的风险。对于品牌广告，由于目标是较长时期内的利益，很难通过数据挖掘的方式直接计算点击价值，而点击率也因为对于用户接触的核心要求变得不是唯一重要的因素。在这种情况下，由需求方自行根据其市场策略与预算控制单位流量的价格并按此计费，是比较合理的交易模式。

二、CPC计费：按点击计费的方式最早产生于搜索广告，并很多为大多数效果广告网络所普遍采用。这种方式是把点击率的估计交给供给方(或者中间市场)，而把点击价值的估计交给需求方。这一分解方式的原理在于：我们相信供给方的通过其收集的大量用户数据，可以根准确地估计点击率；而转化效果是广告商站内的行为，当然他们自己的数据分析体系更能够准确地对其作出评估。

三、CPS(Cost per Sale)/CPA(Cost per Action)/ROI计费：这些都是按照转化付费的地一些变种。应该说这是一种极端的情况，即需求方只按照最后的转化结算，从而很大程度上规避了风险。在这种计费方式下，供给方或中间市场除了估计点击率，还要对点击价值作出估计。这一方式存在两个很明显的问题：一是转化行为并非供给方能够控制，因此也无法进行准确的估计和优化。只有那些转化流程和用户体验相似的广告商组成的广告网络，按转化付费才比较合理，典型的例子比如淘宝直通车；二是存在广告主故意降低转化率，以低成本赚取大量品牌曝光的可能。因此，我们认为这种方式只适合于一些\begin{CJK*}{GBK}{kai}{垂直广告网络(Vertical Network)}\end{CJK*}。

四、CPT(Cost per Time)计费：这是针对大品牌广告主特定的广告活动，将某个广告位以独占式方式交给某广告主，并按独占的时间段收取费用的方式。严格来说这是一种销售方式，而非一种计价模式，因为价格是双方事先约定，无需计费。这种方式在北美市场并非经常采用，不过在中国的门户网站广告中是一种主流模式。CPT还有一种变形，即轮播式CPT，这种方式是将某一广告位的流量按照某一cookie接触到的次数划分成多轮，在其中的若干轮独占式售卖给某广告主，这同样是中国市场很常见的一种售卖方式。CPT这样独占式的售卖虽然有一些额外的品牌效果和橱窗效应产生，但是非常不利于受众定向和程序交易的发展，因而长期看来比例会有下降的趋势。

综合起来看，可以认为对于效果广告，CPC计费方式最有利于发挥供给方和需求方的长处，因而在市场上被广泛接受。而对于品牌广告，由于效果和目的有时不便于直接衡量，可以考虑按照CPM的方式计费。

公式\ref{opt}描述的计算广告核心问题，从优化与系统的两个角度看，分别面临着不同的挑战。从优化的角度看，该问题的特征提取，即对$a,u,c$打标签以方便挖掘的技术，对应着受众定向；如果不考虑全局最优，则主要依靠eCPM估计，特别是CTR预测来完成局部优化；如果考虑到量的限制和投放时即时决策的要求，就产生了在线分配的问题；为了在多方博弈的市场中达到动态平衡时的收益最大化，则需要对定价策略做深入研究；为了更全面地采样整个$(a,u,c)$的空间以便更准确地估计点击率，需要用到强化学习(Reinforcement Learning)中的\begin{CJK*}{GBK}{kai}{探索与利用(Explore and Exploit, E\&E)}\end{CJK*}算法；而在DSP快速发展的今天，推荐算法也被广泛使用在个性化重定向当中。而从系统模块的角度看，我们需要用到实时索引技术服务于广告候选的检索；用到No-SQL 的在线存储技术为投放时提供用户、上下文标签和其他特征；大量使用Hadoop这样的分布式计算平台进行大规模数据挖掘；用到最新的流计算平台实现短时用户行为反馈；以及在广告交易环境下实现高并发、快速响应的的实时竞价接口。

本书并不是专门讨论机器学习和云计算的教程，因此大家不要期望在这里能系统地学习到这些领域的知识。我们的目标是从商业逻辑产生的需求出发，深入解剖问题的本质，并给出一些代表性的解决方案。

\section{计算广告系统架构}

\begin{figure}
\centering
\scalebox{0.6}
{
    \includegraphics[width=1.65\textwidth]{Architect.eps}
}
\end{figure}

整体来看，该系统有三个主体部分：在线的高并发投放引擎(Ad server)，离线的分布式数据处理平台(Grid)，以及用于在线实时反馈的流式处理平台(Stream computing)。 这三部分各司其职，配合完成整个计算广告任务。下面我们来看看按遵照功能划分，这个系统中都有哪些重要的模块：

一、广告投放机，即图中的Ad server：这是接受广告前端Web server发来的请求，完成广告投放决策并返回最后页面片段的主逻辑。

二、广告检索，包括图中的Ad index和Ad retrieval两部分：这部分功能是广告系统的发动机，它包括实时接受广告投放信息，建立倒排索引，以及在线时根据用户与上下文标签从索引中查找广告候选的逻辑。实际上，检索技术的重要性体现在所有Web-scale的技术挑战上，也是大规模计算广告系统的基础。

三、广告排序，包括图中的Ad ranking和Click modeling两部分：这部分是广告系统性能优化的关键，其关键技术在于离线分布式计算平台上的点击率预测模型的训练，当然线上如何高查询模型需要的特征并进行高效计算，也是非常关键的。

四、数据高速公路，即图中的Data highway：这部分完成的功能是将在线投放的数据准实时传输到离线分布式计算平台与流式计算平台上，供后续处理。其他非广告数据与第三方合作数据也可以通过数据高速公路收集起来以进行受众定向。

五、用户日志生成，即图中的Session log generation：从各个渠道收集来日志，需要先整理成以用户ID为key的统一存储格式，我们把这样的日志称为用户日志(Session log)。这样整理的目的。是为了让后续的受众定向过与程更加简单高效。

六、数据仓库，包括ETL, Dashboard和Cube：这些是所有以人为最终接口的数据处理和分析流程的总括。

七、行为定向，包括结构化标签库(Structural label base), Audience targeting, 以及User attributes的cache：这部分完成的是挖掘用户日志，根据日志中的行为给用户打上结构化标签库中某些标签的过程。这部分是计算广告的原材料加工厂，在整个系统中具有最关键的地位。

八、上下文定向，包括半在线页面抓取(Near-line page fetcher)和Page attributes的cache：这部分与行为定向互相配合，负责给上下文页面打上标签，用于在线的广告投放中。

九、定制用户划分，即图中的Customized audience segmentation：由于广告是媒体替广告主完成用户接触，那么有时需要根据广告主的逻辑来划分用户群，这部分也是具有鲜明广告特色的模块。

十、在线反馈：这部分指的是流式管理平台上的一些半实时任务，包括短时的用户标签和短时用户点击反馈。当然，在利用日志完成这些逻辑之前，必须要进行的步骤是反作弊(Anti-spam)与计价(Billing)。

十一、广告管理系统：这部分是广告操作者，即客户执行(Account execute, AE)与广告系统的接口，AE通过广告管理系统定制和调整广告投放，并且与数据仓库交互，获得投放统计数据以支持决策。

十二、实时竞价接口，包括作为需求方时使用的RTBS(RTB for Supply)以及作为供给方时使用的RTBD(RTB for Demand)。

实际上，并不是每一个广告系统都需要以上所有的功能模块。这样的架构图和模块划分，是为了后文比较容易在各种类型的广告系统之间做架构上的对比。另外需要说明，这样一架构描述，主要是根据竞价广告系统的骨架来进行的，对于其他类型的广告系统，虽然概念上也可以套用，可以术语和习惯上并不直接。为了行文的方便和一致性，本书还是会在这一总架构的基础上讨论各种广告系统。

\section{基础知识准备}

本书的写作目的有两重，除了帮助读者理清在线广告市场的产品和商业逻辑，还要对其中关键问题给出业界目前的解决方案。要具体探讨这些解决方案，需要用到相当多的相关学科知识。为了后文讨论的方便，我们把一些需要掌握的基础技术在这里先做一介绍，供希望深入学习计算广告的读者做一些准备。

\subsection{信息检索}

从上面的计算广告系统架构可以看出，广告也采用的是类似搜索的技术框架以获得良好的扩展性。所以我们有必要对信息检索和搜索引擎的基本技术有所了解。不过广告相关的搜索技术范畴远远小于完整的网页搜索引擎，特别是不需要全网爬虫和Page Rank算法的部分。因此，我们重点介绍与广告相关的一些搜索基础技术，包括倒排索引和向量空间模型。

\subsubsection{倒排索引}

倒排索引是现代搜索引擎的核心技术之一，其核心目的，是将从大量文档中查找包含某个词的文档集合这一任务用$O(1)$的时间复杂度完成。此处的$O(1)$ 是相对文档的数目而言，也就是说倒排索引可以实现与文档集大小无关的检索复杂度，这一点对于搜索来说是至关重要的。倒排索引的基本概念可以用下面的例子解释。假设我们有如下的几篇文档：

$D_0=$``谷歌地图之父跳槽Facebook"

$D_1=$``谷歌地图之父加盟Facebook"

$D_2=$``谷歌地图创始人拉斯离开谷歌加盟Facebook"

$D_3=$``谷歌地图创始人跳槽Facebook与Wave项目取消有关"

$D_4=$``谷歌地图创始人拉斯加盟社交网站Facebook" \\
这些文档中包含的关键词(term)有下面这些：\{谷歌，地图，之父，跳槽，Facebook，加盟，创始人，拉斯，离开，与，Wave，项目，取消，有关，社交，网站\}。去掉“与”这样的停止词(stop word)，其对应的倒排索引可以用如下的方式表示：谷歌$\rightarrow \{D_1, D_2, D_3, D_4, D_5\}$，地图$\rightarrow \{D_1, D_2, D_3, D_4, D_5\}$，之父$\rightarrow \{D_1, D_2, D_4, D_5\}$，跳槽$\rightarrow \{D_1, D_4\}$，Facebook$\rightarrow \{D_1, D_2, D_3, D_4, D_5\}$，创始人$\rightarrow \{D_3\}$，加盟$\rightarrow \{D_2, D_3, D_5\}$，拉斯$\rightarrow \{D_3, D_5\}$，离开$\rightarrow \{D_3\}$，Wave$\rightarrow \{D_4\}$，取消$\rightarrow \{D_4\}$，项目$\rightarrow \{D_4\}$，有关$\rightarrow \{D_4\}$，社交$\rightarrow \{D_4\}$，网站$\rightarrow \{D_4\}$。

我们用下面代码中的类结构来表示一个倒排索引。其中Hash Map中的key为关键词即term，典型情况下，该key是string类型，表示一个关键词，但是在后文介绍的布尔表达式检索等场景中，其key的类型可能会发生变化。因此，为了逻辑统一起见，我们引入了模板参数来泛化此处的数据类型。而Hash Map的value就是倒排链，是一个由索引条目 (Entry)组成的链表，链表中的每个元素有两个域，第一个是该条目对应的文档的ID，第二个是一个辅助变量，比如可以用于表示目前关键词在对此文档的TDIDF，在后文的其他索引类型中也会有独特的应用。当然，这种结构只是一个概念上的表达，实际的倒排索引还要存储很多其他信息，但我们为了便于突出主要概念，在本书中采用这样简单的概念性描述。

倒排索引最基本的操作有两项：一是向索引中加入一个新文档，二是给定一个由多个关键词组成的查询(query)时，返回对应的文档集合。我们也在下面的代码中对这两项基本功能的实现做了描述。需要注意的关键点是：在倒排索引中，我们保持每个每个倒排链是有序的，这会在后文的应用中得到具体利用。

\begin{lstlisting}[language={C++}]
typedef pair<int, float> Entry;

template <class TKey> 
class InvIndex : public map<TKey, list<Entry> >
{
 public:
  void add(vector<TKey> & doc, int docID)
    {
      // `遍历document里所有的term`
      for (int w = 0; w < doc.size(); w ++)
        {
          map<TKey, list<TValue> >::iterator it;
          it = index.find(doc[w]);

          // `如果该term的倒排链不存在`
          if (it == index.end())
            {
              list<int> newList;
              index[doc[w]] = newList;
              it = index.find(doc[w]);
            }
        
          // `在倒排链中合适的位置加入新的文档ID`
          list<Entry> entries = it -> second;
          for (list<Entry>::iterator eit = entries.first; e < entries.size(); eite ++)
            if (entries[e].
              entries.push_back(docID);
        }
    }
    
  void retrieve(vector<TKey> & query, vector<int> & docIDs)
    {
      
    }
};
\end{lstlisting}

\subsubsection{向量空间模型}

如果说倒排索引技术是大规模信息检索的基石，那么向量空间模型(Vector Space Model, VSM)则是信息检索中最基础且最重要的文档相似度度量之一。VSM的核心有两点，文档的表示方法和相似度计算方法，我们在下面分别介绍。

首先，文档采用“bag of words”假设，即用每个关键各个词(term)在文档中强度来表示该文档：
\begin{equation}
D=(x_1,x_2, \cdots, x_M)^\top
\end{equation}
其中$x_m$一般采用词表中第$m$个词在$D$中对应的TFIDF(Term frequency - Inverse Document Frequency)值。TFIDF是信息检索中最常用的词强度度量，它可以分解为两个量的乘积，这两个量是词频(Term Frerquency, TF)，即某文档中该词出现的次数，和倒数文档频率(Inverse Document Frequency, IDF)，即该词在所有文档中出现的频繁程度的倒数。IDF的引入，是考虑对那些广泛出现在各个文档中的常用词对主题的鉴别力并不强，因而需要降低其权重。IDF的计算方法有若干种，最常用的形式为：
\begin{equation}
\textrm{IDF}(m)=\log(N / \textrm{DF}(m))
\end{equation}
其中$\textrm{DF}(m)$为词$m$在其中出现的文档的总数目，$N$为总文档数目。在广告应用中如何计算IDF值，在某些情形下需要不同的处理：例如在处理对广告主有价值的竞价标的词时，可以采用所有广告描述，而不是互联网上的网页作为文档集合。相应地，在根据关键词进行广告检索时，也应该使用这种方法得到的TFIDF。

其次，在计算两个文档的相似度时，采用其对应矢量的余弦距离：
\begin{equation}
\cos(D_1, D_2) = \frac{D_1^\top D_2}{\|D_1\|\cdot\|D_1\|}
\end{equation}
余弦距离的好处是当两个矢量归一化不好时，仍然可以得到比较稳健的比较结果。比如有两篇一样的文档，将其中的一篇内容重复一遍，再去计算余弦距离仍然是0，而如果采用其他方式比如欧氏距离，结果就不再是0了。再比如两个人对各种电影打分，甲倾向于给较高的分数，乙倾向于给较低的分数，那么在一组三部电影上，甲给出的分数$\{3.6, 3.6, 4.8\}$，和乙给出的分数$\{3.0, 3.0, 4.0\}$，实际上一致程度相当高，这也可以被余弦距离比较公允地度量出来。

\subsection{最优化方法}

实际互联网应用中与计算相关的最重要的基础技能，是最优化(Optimization)的理论和方法。这个主题讨论的是在给定一个数学上可以明确表达的优化目标以后，如何用系统性的方法和思路找到该目标的最优解。这部分的书籍和文章卷帙浩繁，因此我们有必要从工程的角度出发，整理一下在面临各类目标函数时的一般性思路。

最优化讨论的是给定某个目标函数(Objective Function)，以及该函数自变量的一些约束条件，求该函数最大或最小值的问题。这样的问题可以表示为下面的一般形式：
\begin{eqnarray}\label{opt_gen}
\min f(x) & \nonumber \\
\textrm{s.t.} \quad g_i(x) \leq 0, &\quad i = 1, \cdots, m \\
h_i(x) = 0, & \quad i = 1, \cdots, p\nonumber
\end{eqnarray}
这里$f(x)$是一个关于$x$的目标函数，而$g_i(x)$和$h_i(x)$则对应着一组约束约束条件。根据约束条件以及目标函数的性质不同，最优化的思路也有很大的不同。我们先从没有约束条件的情形开始，这方面的方法是大多数最优化算法的基础。在这种情形下，我们首先根据目标函数$f_0$是否可导，可以把相应的优化问题分成两类，这两类问题的工程解决方案有比较大的差别。

\subsubsection{下降单纯型法}

有些问题中，$f$不可导或者工程上求导代价极大(比如要遍历多次数据或者使用很大内存)。这种情形下，假设函数值是连续的，我们常用的思路是采用不断试探的方法。在自变量为一维的情况下，给定一个初始区间，假设区间内有唯一的最小值，比较自然的方法是按照黄金分割的方法不断缩小区间以得到最小值。这样的方法也可以被推广告高维的情形，对应的算法我们称为下降单纯型法(Downhill Simplex Method)。这一方法有一个更直观的称呼，即阿米巴(Ameoba)变形虫法。简单地讲，将一维空间上用两个点限制的区间不断变形的思路加以推广，在$N$维空间中，我们也可以选择一个$N+1$个点张成的超多面体，或称为单纯型(Simplex)，然后对这一单纯型不断变形以收敛到函数值的最小点。下降单纯型法的示例性代码如下所示。

\subsubsection{梯度方法}

当$f$可以比较容易地求导时，基于梯度的方法是首要选择，这也是我们将要讨论的重点类型。假设$x = {x_1, x_2, \cdots, x_N}$，那么函数$f(x)$在$x^*$点的梯度计算方法如下：
\begin{equation}
\nabla f(x^*) = (\frac{\partial f}{\partial x^*_1}, \frac{\partial f}{\partial x^*_2}, \cdots, \frac{\partial f}{\partial x^*_N})^\top
\end{equation}
梯度的几何意义，是$f(x)$在$x^*$点函数值下降最快的方向，因此是一个与$x$维数相等的矢量。利用梯度的优化方法，就是每次都沿着梯度方向按某步长前进一小步，这样的方法称为梯度下降法(Gradient Descent)。其更新公式为：
\begin{equation}
x \leftarrow x + \epsilon \nabla f(x)
\end{equation}
其中参数$\epsilon$控制着沿梯度方向下降的速度，称为学习率(Learning Rate)。很多工程上的目标函数，都具有可分解的特性，即整个训练集上的梯度可以表示为各个训练样本梯度的和。在这种情况下，一个可行但效率并不太高的并行实现就是将计算梯度的过程分解到各个数据划分上分别完成，然后将各部分的梯度相加并更新参数。显然，这样的计算过程非常容易在map/reduce框架下实现。在在线学习中，梯度下降还有另外一种用法。

\subsubsection{拟牛顿方法}

梯度下降法在实际的工程问题中会遇到一个麻烦，即当函数值对各个自变量归一化不够好时，会陷入Zig-Zag折线更新的困境，这一现象我们可以用下图中的例子来形象地说明。在自变量维数很高时，这一问题尤为严重，因为我们无法一一检查各个自变量的意义，因此在某些维度上缩放尺度不一样是无法避免的。如何缓解这一问题呢？如果函数值向下图中那样呈近似的二次曲面状，那么很自然的思路就是引入二阶导数信息，以迅速探索到函数值的谷底。$f(x)$的的二阶导数是一个$N\times N$的矩阵，其定义为：
\begin{equation}
\nabla^2 f(x^*) = ()
\end{equation}
同时利用梯度和二阶导数做优化，相当于在当前点处进行二阶的泰勒展开，并找到此二次曲面的极小值点，向图*中示意的那样。这样的方法称为牛顿法，其更新公式为：
\begin{equation}
x \leftarrow x + \left[\nabla^2 f(x)\right]^{-1}\nabla f(x)
\end{equation}

牛顿法的每一步都要求一个二次曲面的极小值，显然只有当Hession阵正定时，极小值才存在。不过实际的优化问题中，即使目标函数存在唯一的极小值，也不能保证每一点的Hession都正定，因此一般来说牛顿法是不太可行的。解决这个困难的方法也很简单：我们可以构造一个不太精确，但是保证正定的伪Hession矩阵，用它代替实际的Hession阵来更新参数，这样的方法就是工程上真正实用的拟牛顿法。直观上来看，利用前面几次迭代的函数值和梯度，可以近似地拟合出Hession阵，而随着拟合公式的不同，也就产生了不同的拟牛顿方法。一个例子是由Broyden, Fletcher, Goldfarb, Shanno 四位学者创造的方法，称为BFGS方法。在BFGS方法中，Hession矩阵是迭代更新的，其更新公式如下：

\begin{equation}
H_{i+1} = H_i - \frac{H_i y_i y_i^\top H_i}{y_i^\top H_i y_i} + \frac{s_is_i^\top}{y_i^\top s_i}
\end{equation}
其中$y_i=\nabla_{i+1}-\nabla_i$为前后两次的梯度差，而$s_k=f_{k+1} - f_k$为前后两次的自变量差。

\subsubsection{Trust-Region方法}

拟牛顿法是对目标函数做正定化的近似，而对于自变量不加约束的优化方法。与此思路相对照，存在一种对自变量加一个超球约束，但不对函数曲面做正定化近似的优化方法，这就是Trust-Region方法。通过引入约束，我们可以避免Hession不正定是优化过程的不可控，其每一步优化的目标为解下面的子优化问题：
\begin{eqnarray}
\min x^\top H x + b^\top x\nonumber \\
\textrm{s.t.} \quad x^\top x \leq \delta
\end{eqnarray}
显然，通过牛顿法中的梯度和Hession阵，再引入一个超球的约束，一般的目标函数可以在当前参数点上化成这一形式。由于没有对目标函数的一阶和二阶导做近似，这一方法往往能够更准确地把握下降方向，因此有时能表现出比拟牛顿法更好的收敛性能。虽然目的是为了求解费约束优化问题，Trust-Region方法中每一步的解法，要用到带约束优化问题的技术了，这一点我们在下面介绍。

\subsubsection{带约束优化和拉格朗日法}

带约束优化在工程中非常常见，例如后文中的广告合约量约束下的广告效果优化问题。有关带约束优化最重要的方法论，就是拉格朗日法。具体来说，对公式\label{opt_gen}那样的带约束优化问题，我们可以引入一个拉格朗日对偶函数(Lagrange Dual Function)，或简称对偶函数：
\begin{equation}
g(\lambda, \nu)= \inf_{x} \left(f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{i=1}^p \nu_i h_i(x) \right)
\end{equation}
这里引入的变量$\lambda_{1,\cdots,m}$和$\nu_{1,\cdots,m}$称为拉格朗日乘子，对偶函数是一个关于拉格朗日乘子的函数，

\subsection{统计机器学习}

机器学习(Machine Learning)是近年来得到快速发展和广泛应用的研究领域，它研究的是用数据或以往的经验优化计算机程序或算法的性能。从机器学习的方法论来看，可以大致分为统计的方法和非统计的方法。非统计的方法种类非常多，并且往往最后都归结于一个具体的优化问题，因此笔者认为可以通过深入掌握优化理论和算法，更加有效地把握各种非统计类方法。而统计类机器学习方法，虽然一般性的优化方法也广泛用到，三是也产生了一些在概率模型框架下系统性的方法和思路，下面我们把这一类的方法脉络稍加整理，供大家深入学习时做个简要纲领。

\subsubsection{最大熵原理}

统计机器学习中，指数族形式的分布由于求解的方便性，有非常重要的工程地位，我们先来看一下这一族分布形式产生的原因。要了解指数族形式产生的原因，需要先了解最大熵(Maximum Entropy)的原理。最大熵原理告诉我们，当我们在某些约束条件下选择统计模型时，需要尽可能选择满足这些条件的模型中不确定性最大的那个。如果采用熵作为统计不确定性的度量，这个问题就变成一个在这些约束下优化熵的问题。

\subsubsection{指数族分布}

出于上面对最大熵原理和求解过程的讨论，我们可以了解到指数形式的分布在统计机器学习中的重要地位。实际上，这一族分布是大多数模型在设计时的基本构件，也是我们学习统计机器学习必须掌握的基础。指数族分布的归一化形式(Canonical Form)可以表示为：
\begin{equation}
p(\boldsymbol x | \boldsymbol \theta)=h(\boldsymbol x)g(\boldsymbol \theta)\exp\{\boldsymbol \theta^\top \textrm{u}(\boldsymbol x)\}
\end{equation}
在这一形式中，$\textrm{u}(x)$是对变量$x$的某个固定的矢量变换函数，$\theta$即为指数族分布的参数，而$g(\theta)$为使得概率密度曲线下面积为一的归一化项。

指数族分布在建模上被广泛采用的，是因为一个重要的特性。这一特性是，指数族分布参数的最大似然估计，可以完全由其充分统计量(Sufficient Statistics)得到。这里的充分统计量，指的是训练集上变换函数$\textrm{u}(x)$的统计量，即$\sum_{i=1}^N \textrm{u}(x_i)$。在给定了充分统计量以后，$\theta$的最大似然解可以通过解下式求得：
\begin{equation}\label{ML}
-\nabla \ln g(\theta_{\textrm{ML}}) = \frac{1}{N}\sum_{i=1}^N \textrm{u}(x_i)
\end{equation}
这一概念强调的是，在给定充分统计量以后，最大似然估计过程与数据无关。根据充分统计量的形式，我们很容易得出，无论什么样的指数族分布，我们都只需要遍历一遍数据，就可以得到最大似然解，这一点实际上对应了一个非常简便的map/reduce实现。这也是指数族分布在大数据运算上带给我们的最大便利性，请大家细心体会。

由于指数族的分布形式与最大熵原理的本质联系，这一族的许多重要分布都可以从最大熵的角度加以解释。我们在下表中列举了几种重要的指数族分布形式，以及其主要用于描述的变量类型，希望对大家有所帮助。
\begin{table}
  \caption{若干重要指数族分布形式}
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline 分布 & $\textrm{u}(x)$ & 解释 & 使用场景 & 示例\\
      \hline Gaussian    & $\left[\begin{array}{c}x\\x^2\end{array}\right]$ & {给定均值方差情形下熵最大的分布} & 一般实变量 & \\
      \hline Gamma       & $\left[\begin{array}{c}x\\ \ln x\end{array}\right]$ & 给定均值方差，且$x>0$情形下熵最大的分布 & 非负实变量 &\\
      \hline Beta        & $\left[\begin{array}{c} \ln x\\ \ln (1-x)\end{array}\right]$ & 给定均值方差，且$x\in(0, 1)$情形下熵最大的分布 & 某区间内的实变量 &\\
      \hline Multinomial & $\ln x$ & 给定均值方差，且$x\in(0, 1)$情形下熵最大的分布 & 离散变量 & \\
      \hline
    \end{tabular}
  \end{center}
  \label{table_exp}
\end{table}

从上面的示例中，还可以发现指数族分布的另一个重要特点，那就是这些分布都是单模态(Uni-modal)的。所谓单模态，可以理解为分布从几何形态上来看只有一个峰或者一个谷，这实际上说明了指数族分布虽然数学上使用方便，但其实际上的描述能力是有限的，并不适合于表达多种因素并存的随机变量。

\subsubsection{混合模型和EM算法}

由于指数族分布是单模态的，因而不适用于存在比较复杂的数据分布。为了解决这个问题，同时又能充分利用到指数族分布的一些方便的性质，工程领域产生了采用多个指数族分布叠加的部分来建模的实用方法，即混合模型(Mixture Model)。如果我们用$q(x)$来表示某种指数族分布形式，那么其相应的混合模型为：
\begin{equation}
p(\boldsymbol x | \boldsymbol w, \boldsymbol \Theta)= \sum_{k=1}^K w_k h(\boldsymbol x)g(\boldsymbol \theta_k)\exp\{\boldsymbol \theta_k^\top \textrm{u}(\boldsymbol x)\}
\end{equation}
其中$w = (w_1, \cdots, w_K)$为各个组成分布先验概率，而$\Theta = (\theta_1, \cdots, \theta_K)$表示各个组成分布的参数。这一分布的图模型表达如下图所示：
\begin{figure} \centering
    \setlength{\unitlength}{0.7mm}
    \begin{picture}(170,19)
    \thicklines

    \scaleput(40, 11){\bigcircle{6}}
    \put(39, 4){$\boldsymbol \theta_k$}

    \drawline(43, 11)(67, 11)
    \drawline(63, 10)(67, 11)(63, 12)

    \scaleput(70, 11){\bigcircle{6}}
    \put(68, 5){$\boldsymbol x_n$}

    \drawline(73, 11)(97, 11)
    \drawline(77, 10)(73, 11)(77, 12)

    \scaleput(100, 11){\bigcircle{6}}
    \put(99, 4){$\boldsymbol z_n$}

    \drawline(103, 11)(127, 11)
    \drawline(107, 10)(103, 11)(107, 12)

    \scaleput(130, 11){\bigcircle{6}}
    \put(129, 4){$\boldsymbol w$}

    \put(58,0){\framebox(56,19){}}
    \put(109, 1){$N$}

    \put(28,0){\framebox(26,19){}}
    \put(49, 1){$K$}

    \end{picture}
\caption{混合分布的概率图模型表示}\label{fig_mixture}
\end{figure}

在许多常见的机器学习模型当中，根据多个变量的条件依赖关系，可以用上面这样的有向图模型来比较清晰地表达整体的联合分布。有向图模型表达的信息是：图中的每一个节点代表一个随机变量，而给定了该变量所有入边对应的起始节点后，该变量的分布与其他所有变量都条件无关。需要指出，有向图模型本身只给出了条件依赖关系，并没有明确各条件分布的形式。一般来说，我们在工程中的思路中用图模型来表达我们先验的变量结构关系，然后对每个条件分布选取合适的指数族分布来建模，而混合分布模型就是了解这种工程思路的最典型例子。按照上面的有向图模型表示，我们引入了Multinomial变量$z = (z_1, \cdots, z_K)^T$来明确表示状态，可以把混合分布改写成结构更清晰的表达式：
\begin{equation}
p(\boldsymbol x | \boldsymbol w, \boldsymbol \Theta)= \sum_z \prod_k w_k^{z_k} \left\{ h(\boldsymbol x) g(\boldsymbol \theta_k)\exp\{\boldsymbol \theta_k^\top \textrm{u}(\boldsymbol x)\}\right\}^{z_k}
\end{equation}

在混合模型的最大似然求解过程中，EM(Expectation-Maximization)算法起着非常重要的作用。从上面的概率图模型例子可以看出，除了要求解的参数$\boldsymbol w, \boldsymbol \Theta$ 和观测到的变量$x$，还存在一个变量$z$，我们把这样的变量称为隐变量(Hidden Variable)。EM算法就是为了解决有隐变量存在时的最大似然估计问题的。这是一种迭代的算法，每个迭代又可以分为E-step和M-step，在E-step阶段，我们将参数变量和观测变量都固定，得到隐变量的后验分布；而在M-step，我们将用得到的隐变量的后验分布和观测变量再去更新参数变量。指数族混合分布的EM算法，其E-step和M-step可以写成下式：
\begin{equation}
\textrm{E-step:} \quad \gamma_i(k) \triangleq p(z_k = 1 | \Theta^{\textrm{old}}, w^{\textrm{old}}, x_i) = \frac{w_k^{\textrm{old}} g(\boldsymbol \theta_k^{\textrm{old}})\exp\{\textrm{u}^\top(\boldsymbol x_i)\boldsymbol \theta_k^{\textrm{old}} \}}{\sum_l w_l^{\textrm{old}} g(\boldsymbol \theta_l^{\textrm{old}})\exp\{\textrm{u}^\top(\boldsymbol x_i)\boldsymbol \theta_l^{\textrm{old}} \}}
\end{equation}
\begin{eqnarray}
\textrm{M-step:} \quad &-\nabla \ln g(\theta_k^{\textrm{new}}) = \frac{1}{N}\sum_{i=1}^N \gamma_i(k) \textrm{u}(x_i)\qquad\qquad\qquad\qquad\qquad\qquad\nonumber \\
&w_k^{\textrm{new}} = \frac{1}{N}\sum_{i=1}^N \gamma_i(k) \qquad \qquad \quad \qquad\qquad\qquad\qquad\qquad\qquad
\end{eqnarray}
在混合分布的情形下，这样的一种分解使得许多非指数族分布的模型再进行最大似然估计时，其M-step形式上与简单的指数族分布是一致的，这也使得指数族分布工程上的便利性得以继续发挥。虽然M-step的形势与指数族最大似然估计的形式\ref{ML}非常相近，我们却不宜将等式右边的部分也称为充分统计量，因为这一过程是迭代进行的，需要多次访问数据才能完成最大似然估计，因此，简单地称其为统计量更为准确。

指数族分布的混合模型在工程中的应用同样很广泛，只要是单模态分布不易刻划的数据分布，都可以考虑用某种指数族分布叠加的方式来更精确地建模。常见的混合模型，例如高斯混合模型(Mixture of Gaussians, MoG)，以及PLSI(Probabilistic Latent Semantic Index)，后这可以认为是建立在Multinomial分布基础上的混合模型，在文本主题分析中有着广泛的应用。

需要注意的是，指数族混合分布的EM算法只是EM算法的一种较简单的特殊情况，这一算法广泛应用于各种隐变量存在的统计模型训练中，有关这方面更详细的理论和应用介绍，请大家参看\cite{Dempster}和\cite{Bishop}中更多的介绍。

\subsubsection{贝叶斯学习}

以上讨论的模型参数估计方法，都是在最大似然准则下进行的。最大似然准则，是把模型的参数看成固定的，然后找到使得训练数据上似然值最大的参数，这是一种参数点估计(Point Estimation)的方法。这样的点估计方法，在实际中如果遇到数据样本不足的情形，往往会产生比较大的估计偏差。对此，工程上常常用到贝叶斯学习(Beyes Learning)的方法论。为了介绍贝叶斯学习的基本概念，我们先从下面的贝叶斯公式入手了解一下其中的关键概念：
\begin{equation}\label{Bayes}
    \underbrace{P(\boldsymbol \theta|D)}_{\textrm{posterior}} =
    \frac{\overbrace{p(D|\boldsymbol \theta)}^{\textrm{likelihood}}\overbrace{p(\boldsymbol \theta)}^{\textrm{prior}}}{\underbrace{p(D)}_{\textrm{evidence}}}
\end{equation}
在贝叶斯体系下，模型参数$\theta$不再被认为是固定不变的量，而是服从一定分布的随机变量。在没有数据支持的情况下，我们对其有一个假设性的分布$p(\theta)$，这称为先验分布(Prior)，而在观测到数据集$D=\{x_1, \cdots, x_N\}$以后，根据数据集上表现出来的似然值(Likelihood)$p(D|\theta)$，我们可以得到调整后的后验分布$p(\theta|D)$。先验分布、后验分布和似然值之间的变换关系，就通过上面的贝叶斯公式表达出来。等式右侧的分母项，也是贝叶斯学习中的一个重要概念，称为Evidence，它可以展开表示为$p(D) = \int p(D|\theta) p(\theta) \textrm{d} \theta$。由贝叶斯公式和这些重要概念出发，我们将三种常见的模型估计方法对比在下面的表中：
\begin{table}
  \caption{若干常见模型估计方法}
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
       & 参数估计 & 预测 \\
      \hline
      ML    & $\hat{\boldsymbol \theta}^{\textrm{ML}}_{D} = \arg\max_{\boldsymbol \theta} p(D|\boldsymbol \theta)$ & $p(\boldsymbol o|D) = p(\boldsymbol o|\hat{\boldsymbol \theta}^{\textrm{ML}}_D)$ \\
      \hline
      Bayes & $p(\boldsymbol \theta| D) = p(D|\boldsymbol \theta) p(\boldsymbol \theta)$ & $p(\boldsymbol o|D) = \int p(\boldsymbol o|\boldsymbol \theta) p(\boldsymbol \theta|D) \textrm{d}\boldsymbol \theta$ \\
      \hline
      MAP   & $\hat{\boldsymbol \theta}^{\textrm{MAP}}_{D} = \arg\max_{\boldsymbol \theta} p(\boldsymbol \theta|D)$ & $p(\boldsymbol o| D) = p(\boldsymbol o|\hat{\boldsymbol \theta}^{\textrm{MAP}}_{D})$ \\
      \hline
    \end{tabular}
  \end{center}
  \label{table_est}
\end{table}

概率统计模型有两个主要任务：一是参数估计(Parameter Estimation)，二是预测(Prediction)。其中第二项任务指的是给定一组训练数据$D$，评估某新的观测数据$o$的概率。在最大似然体系中，参数估计是根据似然值最大化得到的点估计，而预测过程就直接利用估计出来的参数计算新的似然值即可。而在贝叶斯体系中，参数的点估计为其后验分布所代替，也就意味着参数在估计结果中具有不确定性(Uncertainty)，于是，在预测过程中，需要用积分的方式将不同参数的可能性都加以考虑，这是两者非常本质的差别。还有一种常见的参数估计方法，称为最大后验概率(Maximum a Posterior, MAP)方法，本质上也是点估计方法，只不过同样引入了先验部分来对参数作规范化，因此，其参数估计形式上是对贝叶斯后验概率求极值，而预测过程则与最大似然情形一样。

\subsubsection{共轭先验}

\subsubsection{变分法}

\subsubsection{经验贝叶斯}

\begin{equation}
    \hat {\boldsymbol \eta} = \arg \max_{\boldsymbol \eta} \int \prod_{i = 1}^K p(D_i|\boldsymbol \theta_i) p(\boldsymbol \theta_i|\boldsymbol \eta) \textrm{d}\boldsymbol \theta_i \nonumber
\end{equation}


\clearpage{\pagestyle{empty}} %\cleardoublepage
%\clearpage{\pagestyle{empty}\cleardoublepage}

\end{CJK*}
